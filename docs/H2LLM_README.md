# H2-LLM Mapping Strategy Implementation

æœ¬æ–‡æ¡£ä»‹ç»äº† H2-LLM æ˜ å°„ç­–ç•¥çš„å®ç°ï¼Œè¯¥ç­–ç•¥åŸºäº ISCA 2025 è®ºæ–‡ï¼š
**"H2-LLM: Hardware-Dataflow Co-Exploration for Heterogeneous Hybrid-Bonding-based Low-Batch LLM Inference"**

## æ¦‚è¿°

H2-LLM æ˜¯ä¸€ä¸ªé’ˆå¯¹è¾¹ç¼˜ç«¯ä½æ‰¹æ¬¡ LLM æ¨ç†çš„å¼‚æ„æ··åˆé”®åˆåŠ é€Ÿå™¨ã€‚æœ¬å®ç°åŒ…å«äº†è®ºæ–‡ä¸­æå‡ºçš„æ ¸å¿ƒæ˜ å°„ç®—æ³•ã€‚

## æ ¸å¿ƒç®—æ³•

### 1. è·¨é€šé“ç®—å­åˆ†åŒºï¼ˆInter-Channel Operator Partitionï¼‰

åŸºäºè®ºæ–‡ç¬¬ 4.2 èŠ‚çš„è§£ææ¨¡å‹ï¼Œè¯¥ç®—æ³•é€šè¿‡æœ€å°åŒ–æ•°æ®ä¼ è¾“å¼€é”€æ¥ç¡®å®šæœ€ä¼˜çš„åˆ†ç‰‡å› å­ã€‚

#### é—®é¢˜å»ºæ¨¡

å¯¹äº GEMM ç®—å­ï¼Œè¾“å…¥å¼ é‡å½¢çŠ¶ä¸º `(M, K)`ï¼Œæƒé‡å¼ é‡å½¢çŠ¶ä¸º `(K, N)`ï¼Œéœ€è¦åœ¨ C ä¸ª NMP é€šé“ä¸Šè¿›è¡Œåˆ†åŒºã€‚

**ä¼˜åŒ–ç›®æ ‡ï¼š** æœ€å°åŒ–æ€»ä¼ è¾“å¼€é”€

```
min_{T_K, T_N} s Ã— M Ã— (K/(T_K Ã— B_s) + N/(T_N Ã— B_l))
```

**çº¦æŸæ¡ä»¶ï¼š**
```
T_K Ã— T_N = C
```

å…¶ä¸­ï¼š
- `s`: å…ƒç´ å¤§å°ï¼ˆå­—èŠ‚ï¼‰
- `M, K, N`: çŸ©é˜µç»´åº¦
- `C`: NMP é€šé“æ•°é‡
- `T_K, T_N`: K å’Œ N ç»´åº¦çš„åˆ†ç‰‡å› å­
- `B_s`: æ¯é€šé“çš„ scatterï¼ˆè¾“å…¥ï¼‰å¸¦å®½
- `B_l`: æ¯é€šé“çš„ loadï¼ˆè¾“å‡ºï¼‰å¸¦å®½

#### è§£æè§£

é€šè¿‡æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ï¼Œå¯ä»¥å¾—åˆ°é—­å¼è§£ï¼š

```
T_K = sqrt(C Ã— K Ã— B_l / (N Ã— B_s))
T_N = C / T_K
```

### 2. æ•°æ®ä¸­å¿ƒæ•°æ®æµæŠ½è±¡ï¼ˆData-Centric Dataflow Abstractionï¼‰

è®ºæ–‡ç¬¬ 5 èŠ‚æå‡ºçš„æ•°æ®ä¸­å¿ƒæ•°æ®æµæŠ½è±¡åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼š

1. **å†…å­˜è®¿é—®ç»„åˆ’åˆ†ï¼ˆMAG Partitionï¼‰**ï¼šå°† transformer å±‚çš„ç®—å­å›¾åˆ’åˆ†ä¸ºå¤šä¸ª Memory Access Groups
2. **ç²—ç²’åº¦ç»‘å®šï¼ˆCoarse-grain Bindingï¼‰**ï¼šå°† Memory Partition Groups åˆ†é…åˆ°å†…å­˜é€šé“å­é›†
3. **ç»†ç²’åº¦ç»‘å®šï¼ˆFine-grain Bindingï¼‰**ï¼šç¡®å®šæ¯ä¸ªç®—å­å±‚ï¼ˆoperator tierï¼‰çš„è¯¦ç»†é€šé“åˆ†é…

## å®ç°è¯´æ˜

### æ–‡ä»¶ç»“æ„

```
matrixmachine/strategy/
â”œâ”€â”€ h2llm_mapping.py          # H2-LLM æ˜ å°„ç­–ç•¥å®ç°
â””â”€â”€ ...
docs/
â””â”€â”€ H2LLM_README.md           # æœ¬æ–‡æ¡£
```

### ä¸»è¦ç±»

#### `H2LLMTilingStrategy`

å®ç°äº†è®ºæ–‡ä¸­çš„è·¨é€šé“ç®—å­åˆ†åŒºç®—æ³•ã€‚

**å‚æ•°ï¼š**
- `element_size`: çŸ©é˜µå…ƒç´ å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œé»˜è®¤ä¸º 2ï¼ˆFP16ï¼‰

**æ³¨æ„ï¼š** å¸¦å®½å‚æ•°ä¼šè‡ªåŠ¨ä» chip å¯¹è±¡çš„ ChipSpec ä¸­æå–ï¼š
- `scatter_bandwidth`: ä» `chip.spec.die_spec.input_bandwidth` æå–
- `load_bandwidth`: ä» `chip.spec.die_spec.output_bandwidth` æå–

**ä¸»è¦æ–¹æ³•ï¼š**
- `create_mapping(matrix_shape, chip)`: åˆ›å»ºä¼˜åŒ–çš„æ˜ å°„
- `find_optimal_mapping(matrix_shape, chip)`: åˆ›å»ºæ˜ å°„å¹¶è¯„ä¼°æ€§èƒ½

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
from matrixmachine.core.description import MatrixShape
from matrixmachine.workload.hardware.h2llm import create_h2llm_chip
from matrixmachine.strategy.h2llm_mapping import H2LLMTilingStrategy

# åˆ›å»ºç¡¬ä»¶é…ç½®
chip = create_h2llm_chip(die_count=8)

# åˆ›å»ºç­–ç•¥ï¼ˆå¸¦å®½å‚æ•°è‡ªåŠ¨ä» chip ä¸­æå–ï¼‰
strategy = H2LLMTilingStrategy()

# åˆ›å»ºæ˜ å°„
matrix = MatrixShape(rows=4096, cols=4096, batch_size=4)
result = strategy.find_optimal_mapping(matrix, chip)

print(f"Latency: {result.latency}")
print(f"Utilization: {result.get_compute_utilization():.2%}")
```

#### `H2LLMDataCentricStrategy`

æ‰©å±•åŸºç¡€åˆ†ç‰‡ç­–ç•¥ï¼Œå®ç°æ•°æ®ä¸­å¿ƒæ•°æ®æµæŠ½è±¡ï¼ˆå½“å‰ç‰ˆæœ¬ä½œä¸ºå ä½ç¬¦ï¼‰ã€‚

**æ³¨æ„ï¼š** å®Œæ•´çš„æ•°æ®ä¸­å¿ƒç­–ç•¥å®ç°éœ€è¦ç®—å­å›¾åˆ†æå’Œå¼‚æ„æ‰§è¡Œæ˜ å°„ï¼Œè¿™è¶…å‡ºäº†å½“å‰å®ç°çš„èŒƒå›´ã€‚

## âœ… å·²å®Œæˆçš„æ›´æ–°

### ä»£ç æ”¹è¿›

**ä¸»è¦å˜æ›´ï¼š**
- âœ… ç§»é™¤äº† `scatter_bandwidth` å’Œ `load_bandwidth` ä½œä¸ºç±»å±æ€§
- âœ… å¸¦å®½å‚æ•°ç°åœ¨è‡ªåŠ¨ä» `Chip` å¯¹è±¡çš„ `ComputeDieSpec` ä¸­æå–
- âœ… æ•°æ®æ ¼å¼å›ºå®šä¸º FP16ï¼ˆ`element_size=2.0`ï¼‰

**ä¿®æ”¹çš„æ–‡ä»¶ï¼š**
1. `matrixmachine/strategy/h2llm_mapping.py`:
   - `H2LLMTilingStrategy` ç±»ç®€åŒ–ï¼Œåªä¿ç•™ `element_size` å‚æ•°
   - `create_mapping()` æ–¹æ³•è‡ªåŠ¨æå–å¸¦å®½ï¼š
     ```python
     first_die = list(chip.compute_dies.values())[0]
     scatter_bandwidth = first_die.input_bandwidth  # GB/s
     load_bandwidth = first_die.output_bandwidth    # GB/s
     ```
   - `_calculate_optimal_tiling()` æ–¹æ³•æ¥æ”¶å¸¦å®½ä½œä¸ºå‚æ•°

2. `example_h2llm_mapping.py`:
   - ç®€åŒ–ç­–ç•¥åˆ›å»ºï¼š`h2llm_strategy = H2LLMTilingStrategy()`
   - ç§»é™¤äº†æ‰‹åŠ¨ä¼ å…¥å¸¦å®½å‚æ•°çš„ä»£ç 

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### è‡ªåŠ¨å‚æ•°æå–
```python
# æ—§æ–¹å¼ï¼ˆéœ€è¦æ‰‹åŠ¨ä¼ å…¥ï¼‰
strategy = H2LLMTilingStrategy(
    scatter_bandwidth=12.5,
    load_bandwidth=12.5,
)

# æ–°æ–¹å¼ï¼ˆè‡ªåŠ¨æå–ï¼‰
strategy = H2LLMTilingStrategy()  # ä» chip è‡ªåŠ¨æå–å¸¦å®½
```

### å¸¦å®½æå–é€»è¾‘
```python
# ä» chip.spec ç›´æ¥æå–å¸¦å®½å‚æ•°
die_spec = chip.spec.die_spec
scatter_bandwidth = die_spec.input_bandwidth   # è¾“å…¥å¸¦å®½
load_bandwidth = die_spec.output_bandwidth     # è¾“å‡ºå¸¦å®½
```

### å‡è®¾å’Œçº¦å®š
- **åŒæ„å‡è®¾**ï¼šæ‰€æœ‰ compute dies å…·æœ‰ç›¸åŒçš„é…ç½®
- **FP16 æ•°æ®æ ¼å¼**ï¼šå…ƒç´ å¤§å°å›ºå®šä¸º 2 å­—èŠ‚
- **å¸¦å®½å•ä½**ï¼šGB/s

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ç”¨æ³•
```python
from matrixmachine.core.description import MatrixShape
from matrixmachine.workload.hardware.h2llm import create_h2llm_chip
from matrixmachine.strategy.h2llm_mapping import H2LLMTilingStrategy

# åˆ›å»º H2-LLM ç¡¬ä»¶
chip = create_h2llm_chip(die_count=8)

# åˆ›å»ºç­–ç•¥ï¼ˆæ— éœ€æ‰‹åŠ¨é…ç½®å¸¦å®½ï¼‰
strategy = H2LLMTilingStrategy()

# åˆ›å»ºå¹¶è¯„ä¼°æ˜ å°„
matrix = MatrixShape(rows=4096, cols=4096, batch_size=4)
result = strategy.find_optimal_mapping(matrix, chip)

print(f"Latency: {result.latency:.2f} cycles")
print(f"Utilization: {result.get_compute_utilization():.2%}")
```

### è‡ªå®šä¹‰é…ç½®

```python
from matrixmachine.strategy.h2llm_mapping import H2LLMTilingStrategy
from matrixmachine.workload.hardware.h2llm import create_h2llm_chip
from matrixmachine.core.description import MatrixShape

# è‡ªå®šä¹‰ç¡¬ä»¶é…ç½®
chip = create_h2llm_chip(die_count=4)  # 4 ä¸ª compute dies

# åˆ›å»ºç­–ç•¥ï¼ˆå¸¦å®½å‚æ•°è‡ªåŠ¨ä» chip æå–ï¼‰
strategy = H2LLMTilingStrategy()

# æµ‹è¯•ä¸åŒçš„çŸ©é˜µå¤§å°
matrices = [
    MatrixShape(rows=1024, cols=1024, batch_size=1),
    MatrixShape(rows=2048, cols=2048, batch_size=4),
    MatrixShape(rows=4096, cols=4096, batch_size=16),
]

for matrix in matrices:
    result = strategy.find_optimal_mapping(matrix, chip)
    if result:
        print(f"Matrix: {matrix.rows}Ã—{matrix.cols}Ã—{matrix.batch_size}")
        print(f"  Latency: {result.latency:.2f}")
        print(f"  Utilization: {result.get_compute_utilization():.2%}")
```

### å®Œæ•´ç¤ºä¾‹
```bash
# è¿è¡Œç¤ºä¾‹ç¨‹åº
python example_h2llm_mapping.py
```

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### H2-LLM ç®—æ³•ï¼ˆè®ºæ–‡ç¬¬ 4.2 èŠ‚ï¼‰

**ä¼˜åŒ–ç›®æ ‡ï¼š**
```
min_{T_K, T_N} s Ã— M Ã— (K/(T_K Ã— B_s) + N/(T_N Ã— B_l))
s.t. T_K Ã— T_N = C
```

**è§£æè§£ï¼š**
```
T_K = sqrt(C Ã— K Ã— B_l / (N Ã— B_s))
T_N = C / T_K
```

**å‚æ•°è¯´æ˜ï¼š**
- `s`: å…ƒç´ å¤§å°ï¼ˆ2 å­—èŠ‚ï¼ŒFP16ï¼‰
- `M, K, N`: çŸ©é˜µç»´åº¦
- `C`: compute die æ•°é‡
- `B_s`: scatter å¸¦å®½ï¼ˆè‡ªåŠ¨ä» `input_bandwidth` æå–ï¼‰
- `B_l`: load å¸¦å®½ï¼ˆè‡ªåŠ¨ä» `output_bandwidth` æå–ï¼‰

## ç®—æ³•ç‰¹ç‚¹

### 1. è§£æè§£ä¼˜åŠ¿

- **å¿«é€Ÿè®¡ç®—**ï¼šä½¿ç”¨é—­å¼è§£ï¼Œæ— éœ€è¿­ä»£æœç´¢
- **ç†è®ºæœ€ä¼˜**ï¼šåŸºäºæ•°å­¦æ¨å¯¼çš„æœ€ä¼˜è§£
- **å‚æ•°æ•æ„Ÿ**ï¼šè€ƒè™‘äº†å¸¦å®½æ¯”ä¾‹å’ŒçŸ©é˜µç»´åº¦

### 2. åˆ†ç‰‡ç­–ç•¥

- **M ç»´åº¦ä¸åˆ†ç‰‡**ï¼šé¿å…æƒé‡å¤åˆ¶ï¼ˆè®ºæ–‡ç¬¬ 4.2 èŠ‚ï¼‰
- **K/N ç»´åº¦åˆ†ç‰‡**ï¼šæ ¹æ®è§£ææ¨¡å‹ç¡®å®šæœ€ä¼˜åˆ†ç‰‡
- **æ‰¹æ¬¡ç»´åº¦å¤„ç†**ï¼šå½“æœ‰é¢å¤–å®¹é‡æ—¶åˆ†å‰²æ‰¹æ¬¡ç»´åº¦

### 3. è´Ÿè½½å‡è¡¡

- ä½¿ç”¨è½®è¯¢ï¼ˆround-robinï¼‰æ–¹å¼åˆ†é… tiles
- ç¡®ä¿æ¯ä¸ª compute die è·å¾—å‡è¡¡çš„å·¥ä½œè´Ÿè½½

## ğŸ“Š è®ºæ–‡ç»“æœ

æ ¹æ®è®ºæ–‡ï¼ˆç¬¬ 7 èŠ‚ï¼‰çš„å®éªŒç»“æœï¼š

- **æ€§èƒ½æå‡**ï¼šç›¸æ¯”ç°æœ‰ in-die NMP æ¶æ„ï¼Œå®ç° 2.72Ã— çš„å‡ ä½•å¹³å‡åŠ é€Ÿ
- **èƒ½æ•ˆæå‡**ï¼šå®ç° 1.48Ã— çš„å‡ ä½•å¹³å‡èƒ½æ•ˆæå‡
- **åœºæ™¯è¦†ç›–**ï¼šåœ¨ä¸åŒæ‰¹æ¬¡å¤§å°ï¼ˆ1/4/16ï¼‰å’Œä¸åŒåº”ç”¨åœºæ™¯ä¸‹å‡è¡¨ç°ä¼˜å¼‚

### æµ‹è¯•åœºæ™¯ï¼ˆè®ºæ–‡ Table 1ï¼‰

| åº”ç”¨åœºæ™¯ | æ•°æ®é›† | å¹³å‡æç¤ºé•¿åº¦ | å¹³å‡è§£ç é•¿åº¦ |
|---------|-------|------------|------------|
| ä»£ç è¡¥å…¨ | HumanEval | 157 | 67 |
| èŠå¤©æœºå™¨äºº | ShareGPT | 783 | 209 |
| ä¸Šä¸‹æ–‡ç†è§£ | LongBench | 1886 | 97 |
| é—®ç­” | LooGLE | 1971 | 17 |

## ğŸ“Š é¢„æœŸæ•ˆæœï¼ˆæ¥è‡ªè®ºæ–‡ï¼‰

æ ¹æ® ISCA 2025 è®ºæ–‡çš„å®éªŒç»“æœï¼š
- **2.72Ã—** å‡ ä½•å¹³å‡åŠ é€Ÿï¼ˆç›¸æ¯” in-die NMPï¼‰
- **1.48Ã—** å‡ ä½•å¹³å‡èƒ½æ•ˆæå‡
- é€‚ç”¨äºä¸åŒæ‰¹æ¬¡å¤§å°ï¼ˆ1/4/16ï¼‰

## ğŸš€ ä¸‹ä¸€æ­¥

å»ºè®®çš„æ‰©å±•æ–¹å‘ï¼š
1. **å®Œæ•´çš„æ•°æ®ä¸­å¿ƒæ•°æ®æµ**ï¼šå®ç° MAG åˆ’åˆ†ã€GCMap å’Œ OCMap
2. **å¼‚æ„ç¡¬ä»¶æ”¯æŒ**ï¼šå¤„ç†ä¸åŒ compute dies å…·æœ‰ä¸åŒé…ç½®çš„æƒ…å†µ
3. **åŠ¨æ€è°ƒä¼˜**ï¼šæ ¹æ®è¿è¡Œæ—¶åé¦ˆè°ƒæ•´æ˜ å°„ç­–ç•¥
4. **å¤šç²¾åº¦æ”¯æŒ**ï¼šæ”¯æŒ FP32ã€INT8 ç­‰ä¸åŒæ•°æ®æ ¼å¼

## æ‰©å±•æ–¹å‘

### çŸ­æœŸæ‰©å±•

1. **å®Œæ•´çš„æ•°æ®ä¸­å¿ƒæ•°æ®æµ**ï¼šå®ç° MAG åˆ’åˆ†ã€GCMap å’Œ OCMap
2. **ç®—å­èåˆæ”¯æŒ**ï¼šæ”¯æŒå¤šä¸ªç®—å­çš„è”åˆæ˜ å°„
3. **åŠ¨æ€è°ƒä¼˜**ï¼šæ ¹æ®è¿è¡Œæ—¶åé¦ˆè°ƒæ•´æ˜ å°„ç­–ç•¥

### é•¿æœŸæ‰©å±•

1. **è‡ªåŠ¨ DSE æ¡†æ¶**ï¼šå®ç°è®ºæ–‡ç¬¬ 6 èŠ‚çš„è®¾è®¡ç©ºé—´æ¢ç´¢æ¡†æ¶
2. **å¤šæ¨¡å‹æ”¯æŒ**ï¼šæ”¯æŒä¸åŒçš„ transformer å˜ä½“ï¼ˆMHA/GQA/MQAï¼‰
3. **æ€§èƒ½å»ºæ¨¡**ï¼šé›†æˆæ›´ç²¾ç¡®çš„æ€§èƒ½é¢„æµ‹æ¨¡å‹

## ğŸ“ æ–‡ä»¶æ¸…å•

```
matrixmachine/strategy/
â”œâ”€â”€ h2llm_mapping.py          # æ ¸å¿ƒå®ç°ï¼ˆå·²æ›´æ–°ï¼‰
â””â”€â”€ __init__.py               # æ¨¡å—å¯¼å‡ºï¼ˆå·²æ›´æ–°ï¼‰

docs/
â””â”€â”€ H2LLM_README.md           # æœ¬æ–‡æ¡£

example_h2llm_mapping.py      # ç¤ºä¾‹ç¨‹åºï¼ˆå·²æ›´æ–°ï¼‰
```

## âœ¨ å…³é”®ä¼˜åŠ¿

1. **æ›´ç®€æ´çš„ API**ï¼šç”¨æˆ·æ— éœ€å…³å¿ƒå¸¦å®½é…ç½®ç»†èŠ‚
2. **è‡ªåŠ¨é€‚é…**ï¼šæ ¹æ®ç¡¬ä»¶é…ç½®è‡ªåŠ¨è°ƒæ•´
3. **ç±»å‹å®‰å…¨**ï¼šFP16 æ ¼å¼å›ºå®šï¼Œé¿å…é…ç½®é”™è¯¯
4. **æ˜“äºç»´æŠ¤**ï¼šå‡å°‘äº†é…ç½®å‚æ•°ï¼Œé™ä½å‡ºé”™æ¦‚ç‡

## å‚è€ƒæ–‡çŒ®

```bibtex
@inproceedings{h2llm2025,
  title={H2-LLM: Hardware-Dataflow Co-Exploration for Heterogeneous Hybrid-Bonding-based Low-Batch LLM Inference},
  author={Li, Cong and Yin, Yihan and Wu, Xintong and Zhu, Jingchen and Gao, Zhutianya and Niu, Dimin and Wu, Qiang and Si, Xin and Xie, Yuan and Zhang, Chen and Sun, Guangyu},
  booktitle={Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA)},
  year={2025}
}
```

## ç›¸å…³é“¾æ¥

- è®ºæ–‡ PDF: `papers/H2-LLM.pdf`
- å¼€æºä»£ç : https://github.com/leesong/H2-LLM-ISCA-2025
- ç¤ºä¾‹ä»£ç : `example_h2llm_mapping.py`

## è´¡çŒ®è€…

å®ç°åŸºäº MatrixMachine æ¡†æ¶å’Œ H2-LLM è®ºæ–‡ã€‚

---

**å®ç°å®Œæˆæ—¥æœŸï¼š** 2025-01-XX
**åŸºäºè®ºæ–‡ï¼š** H2-LLM (ISCA 2025)
**æ¡†æ¶ç‰ˆæœ¬ï¼š** MatrixMachine v0.1